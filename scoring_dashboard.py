
"""
Scoring & Hiring Decision + Results Dashboard - PROPORTIONAL SCORING VERSION
- Scores calculated proportionally based on questions answered
- Updated evaluation matrix: Fluency 25%, Accuracy 25%, Confidence 20%, Grammar 10%, Vocabulary 10%, Coherence 7%, Fillers 3%
- Violation penalty: -5% per violation
"""

import streamlit as st
import numpy as np
import pandas as pd
import os
import time

class ScoringDashboard:
    """Handles scoring, hiring decisions, and results visualization - PROPORTIONAL SCORING"""
    
    def __init__(self):
        """Initialize scoring dashboard"""
        pass
    
    def is_valid_transcript(self, text):
        """Check if transcript is valid"""
        if not text or not text.strip():
            return False
        invalid_markers = ["[Could not understand audio]", "[Speech recognition service unavailable]", 
                          "[Error", "[No audio]", "Audio not clear"]
        return not any(marker in text for marker in invalid_markers)
    
    def decide_hire(self, result):
        """
        Make hiring decision - UPDATED WEIGHTS
        Fluency 25%, Accuracy 25%, Confidence 20%, Grammar 10%, Vocabulary 10%, Coherence 7%, Fillers 3%
        """
        reasons = []
        conf = result.get("emotion_scores", {}).get("confidence", 0)
        nerv = result.get("emotion_scores", {}).get("nervousness", 0)
        acc = result.get("accuracy", 0) or 0
        flu = result.get("fluency", 0) or 0
        fluency_level = result.get("fluency_level", "No Data")
        violations = result.get("violations", [])
        
        fluency_detailed = result.get("fluency_detailed", {})
        speech_rate = fluency_detailed.get("speech_rate", 0)
        speech_rate_normalized = fluency_detailed.get("speech_rate_normalized", 0)
        grammar_score = fluency_detailed.get("grammar_score", 0)
        grammar_errors = fluency_detailed.get("grammar_errors", 0)
        lexical_diversity = fluency_detailed.get("lexical_diversity", 0)
        coherence_score = fluency_detailed.get("coherence_score", 0)
        filler_count = fluency_detailed.get("filler_count", 0)
        filler_ratio = fluency_detailed.get("filler_ratio", 0)
        pause_ratio = fluency_detailed.get("pause_ratio", 0)
        num_pauses = fluency_detailed.get("num_pauses", 0)
        
        has_valid_answer = self.is_valid_transcript(result.get("transcript", ""))
        
        # Check for no valid response
        if not has_valid_answer:
            return "‚ùå No Valid Response", [
                "‚ùå No valid audio response detected",
                "‚ö†Ô∏è Please ensure you speak clearly during recording"
            ]
        
        # Check for violations
        if len(violations) > 0:
            reasons.append(f"‚ö†Ô∏è {len(violations)} violation(s) detected - under review")
        
        # Calculate positive score
        pos = 0
        
        # === CONFIDENCE (20% weight) ===
        if conf >= 75:
            pos += 2.5
            reasons.append(f"‚úÖ Excellent confidence ({conf}%)")
        elif conf >= 60:
            pos += 2
            reasons.append(f"‚úÖ High confidence ({conf}%)")
        elif conf >= 45:
            pos += 1
            reasons.append(f"‚úì Moderate confidence ({conf}%)")
        else:
            reasons.append(f"‚ö†Ô∏è Low confidence ({conf}%)")
        
        # === ANSWER ACCURACY (25% weight) ===
        if acc >= 75:
            pos += 3
            reasons.append(f"‚úÖ Excellent answer relevance ({acc}%)")
        elif acc >= 60:
            pos += 2
            reasons.append(f"‚úÖ Strong answer relevance ({acc}%)")
        elif acc >= 45:
            pos += 1
            reasons.append(f"‚úì Acceptable answer ({acc}%)")
        else:
            reasons.append(f"‚ö†Ô∏è Low answer relevance ({acc}%)")
        
        # === FLUENCY (25% weight) ===
        if fluency_level == "Excellent":
            pos += 4
            reasons.append(f"‚úÖ Outstanding fluency ({flu}% - {fluency_level})")
        elif fluency_level == "Fluent":
            pos += 3
            reasons.append(f"‚úÖ Strong fluency ({flu}% - {fluency_level})")
        elif fluency_level == "Moderate":
            pos += 1.5
            reasons.append(f"‚úì Moderate fluency ({flu}% - {fluency_level})")
        else:
            reasons.append(f"‚ö†Ô∏è Fluency needs improvement ({flu}% - {fluency_level})")
        
        # === SPEECH RATE ===
        if speech_rate_normalized >= 0.9:
            reasons.append(f"‚úÖ Optimal speech rate ({speech_rate:.0f} WPM)")
        elif speech_rate_normalized >= 0.7:
            reasons.append(f"‚úì Good speech rate ({speech_rate:.0f} WPM)")
        elif speech_rate > 180:
            reasons.append(f"‚ö†Ô∏è Speaking too fast ({speech_rate:.0f} WPM - may indicate nervousness)")
        elif speech_rate < 120:
            reasons.append(f"‚ö†Ô∏è Speaking too slow ({speech_rate:.0f} WPM)")
        
        # === GRAMMAR (10% weight) ===
        if grammar_score >= 85:
            pos += 1
            reasons.append(f"‚úÖ Excellent grammar ({grammar_score:.0f}% - {grammar_errors} errors)")
        elif grammar_score >= 70:
            reasons.append(f"‚úì Good grammar ({grammar_score:.0f}% - {grammar_errors} errors)")
        elif grammar_score >= 55:
            reasons.append(f"‚úì Acceptable grammar ({grammar_score:.0f}% - {grammar_errors} errors)")
        else:
            reasons.append(f"‚ö†Ô∏è Grammar needs improvement ({grammar_score:.0f}% - {grammar_errors} errors)")
        
        # === VOCABULARY (10% weight) ===
        if lexical_diversity >= 65:
            pos += 1
            reasons.append(f"‚úÖ Rich vocabulary ({lexical_diversity:.0f}%)")
        elif lexical_diversity >= 50:
            reasons.append(f"‚úì Good vocabulary variety ({lexical_diversity:.0f}%)")
        else:
            reasons.append(f"‚ö†Ô∏è Limited vocabulary ({lexical_diversity:.0f}%)")
        
        # === COHERENCE (7% weight) ===
        if coherence_score >= 75:
            pos += 0.5
            reasons.append(f"‚úÖ Highly coherent response ({coherence_score:.0f}%)")
        elif coherence_score >= 60:
            reasons.append(f"‚úì Coherent response ({coherence_score:.0f}%)")
        
        # === FILLER WORDS (3% weight) ===
        if filler_count == 0:
            pos += 0.5
            reasons.append(f"‚úÖ No filler words detected")
        elif filler_count <= 2:
            reasons.append(f"‚úì Minimal filler words ({filler_count})")
        elif filler_count <= 5:
            reasons.append(f"‚ö†Ô∏è Some filler words ({filler_count})")
        else:
            pos -= 0.5
            reasons.append(f"‚ö†Ô∏è Excessive filler words ({filler_count} - impacts fluency)")
        
        # === PAUSES ===
        if pause_ratio < 0.15:
            reasons.append(f"‚úÖ Good speech flow ({pause_ratio*100:.1f}% pauses)")
        elif pause_ratio < 0.25:
            reasons.append(f"‚úì Acceptable pauses ({pause_ratio*100:.1f}%)")
        else:
            reasons.append(f"‚ö†Ô∏è Frequent pauses ({pause_ratio*100:.1f}% - may indicate hesitation)")
        
        # === NERVOUSNESS PENALTY ===
        if nerv >= 60:
            pos -= 1.5
            reasons.append(f"‚ö†Ô∏è Very high nervousness ({nerv}%)")
        elif nerv >= 45:
            pos -= 0.5
            reasons.append(f"‚ö†Ô∏è High nervousness ({nerv}%)")
        
        # === VIOLATION PENALTY ===
        if len(violations) > 0:
            violation_penalty = len(violations) * 1.5
            pos -= violation_penalty
        
        # === FINAL DECISION ===
        if len(violations) >= 3:
            decision = "‚ùå Disqualified"
            reasons.insert(0, "üö´ Multiple serious violations - integrity compromised")
        elif pos >= 9:
            decision = "‚úÖ Strong Hire"
            reasons.insert(0, "üéØ Exceptional candidate - outstanding communication and competence")
        elif pos >= 7:
            decision = "‚úÖ Hire"
            reasons.insert(0, "üëç Strong candidate with excellent communication skills")
        elif pos >= 5:
            decision = "‚ö†Ô∏è Maybe"
            reasons.insert(0, "ü§î Moderate potential - further evaluation recommended")
        elif pos >= 3:
            decision = "‚ö†Ô∏è Weak Maybe"
            reasons.insert(0, "üìä Below average - significant concerns present")
        else:
            decision = "‚ùå No"
            reasons.insert(0, "‚ùå Not recommended - needs substantial improvement")
        
        return decision, reasons
    
    def display_violation_images(self, violations):
        """Display violation images"""
        if not violations:
            return
        
        st.markdown("### üö® Violation Evidence")
        
        for idx, violation in enumerate(violations):
            violation_reason = violation.get('reason', 'Unknown violation')
            violation_time = violation.get('timestamp', 0)
            image_path = violation.get('image_path')
            
            col1, col2 = st.columns([2, 3])
            
            with col1:
                if image_path and os.path.exists(image_path):
                    st.image(image_path, caption=f"Violation #{idx+1}", use_container_width=True)
                else:
                    st.error("Image not available")
            
            with col2:
                st.markdown(f"""
                **Violation #{idx+1}**
                
                - **Type:** {violation_reason}
                - **Time:** {violation_time:.1f}s into question
                - **Status:** ‚ö†Ô∏è Flagged for review
                """)
            
            if idx < len(violations) - 1:
                st.markdown("---")
    
    def display_immediate_results(self, result):
        """Display immediate results - ACCURATE METRICS ONLY"""
        st.markdown("---")
        st.subheader("üìä Question Results")
        
        # Show accuracy badge
        improvements = result.get("improvements_applied", {})
        if improvements.get('no_fake_metrics'):
            st.success("‚úÖ **All metrics verified accurate** - No fake scores included")
        
        col_v, col_r = st.columns([2, 3])
        
        with col_v:
            if os.path.exists(result.get('video_path', '')):
                st.video(result['video_path'])
        
        with col_r:
            # Show violations
            violations = result.get('violations', [])
            if violations:
                st.error(f"‚ö†Ô∏è **{len(violations)} Violation(s) Detected**")
                with st.expander("View Violation Evidence", expanded=True):
                    self.display_violation_images(violations)
            
            st.write("**üîç Transcript:**")
            if self.is_valid_transcript(result.get('transcript', '')):
                st.text_area("", result['transcript'], height=100, disabled=True, label_visibility="collapsed")
            else:
                st.error(result.get('transcript', 'No transcript'))
            
            # Main metrics (4 columns - NO fake metrics)
            m1, m2, m3, m4 = st.columns(4)
            with m1:
                st.metric("üòä Confidence", f"{result.get('emotion_scores', {}).get('confidence', 0)}%")
            with m2:
                st.metric("üìä Accuracy", f"{result.get('accuracy', 0)}%",
                         help="Content similarity to ideal answer")
            with m3:
                fluency_level = result.get('fluency_level', 'N/A')
                st.metric("üó£Ô∏è Fluency", f"{result.get('fluency', 0)}%", delta=fluency_level)
            with m4:
                filler_count = result.get('filler_count', 0)
                filler_status = "‚úÖ" if filler_count <= 2 else "‚ö†Ô∏è"
                st.metric(f"{filler_status} Filler Words", filler_count,
                         help="um, uh, like, etc.")
            
            # Enhanced fluency breakdown
            fluency_detailed = result.get('fluency_detailed', {})
            if fluency_detailed:
                st.markdown("---")
                st.markdown("**üìà Detailed Fluency Analysis (All Accurate):**")
                
                fc1, fc2, fc3, fc4 = st.columns(4)
                with fc1:
                    speech_rate = fluency_detailed.get('speech_rate', 0)
                    speech_rate_norm = fluency_detailed.get('speech_rate_normalized', 0)
                    ideal = "‚úÖ" if speech_rate_norm >= 0.9 else ("‚úì" if speech_rate_norm >= 0.7 else "‚ö†Ô∏è")
                    st.metric(f"{ideal} Speech Rate", f"{speech_rate:.0f} WPM",
                             delta=f"Quality: {speech_rate_norm:.2f}")
                with fc2:
                    pause_ratio = fluency_detailed.get('pause_ratio', 0)
                    num_pauses = fluency_detailed.get('num_pauses', 0)
                    pause_status = "‚úÖ" if pause_ratio < 0.2 else ("‚úì" if pause_ratio < 0.3 else "‚ö†Ô∏è")
                    st.metric(f"{pause_status} Pauses", f"{num_pauses}",
                             delta=f"{pause_ratio*100:.1f}% time")
                with fc3:
                    grammar = fluency_detailed.get('grammar_score', 0)
                    errors = fluency_detailed.get('grammar_errors', 0)
                    grammar_status = "‚úÖ" if grammar >= 85 else ("‚úì" if grammar >= 70 else "‚ö†Ô∏è")
                    st.metric(f"{grammar_status} Grammar", f"{grammar:.0f}%",
                             delta=f"{errors} errors")
                with fc4:
                    diversity = fluency_detailed.get('lexical_diversity', 0)
                    div_status = "‚úÖ" if diversity >= 65 else ("‚úì" if diversity >= 50 else "‚ö†Ô∏è")
                    st.metric(f"{div_status} Vocabulary", f"{diversity:.0f}%",
                             help="Unique meaningful words")
            
            st.markdown("---")
            decision = result.get('hire_decision', 'N/A')
            if "‚úÖ" in decision:
                st.markdown(f'<div class="success-box"><h3>{decision}</h3></div>', unsafe_allow_html=True)
            elif "‚ö†Ô∏è" in decision:
                st.markdown(f'<div class="warning-box"><h3>{decision}</h3></div>', unsafe_allow_html=True)
            else:
                st.markdown(f'<div class="error-box"><h3>{decision}</h3></div>', unsafe_allow_html=True)
            
            st.write("**Reasons:**")
            for r in result.get('hire_reasons', []):
                st.write(f"‚Ä¢ {r}")
    
    def display_performance_overview(self, results):
        """
        Display performance overview - PROPORTIONAL SCORING
        NEW WEIGHTS: Fluency 25%, Accuracy 25%, Confidence 20%, Grammar 10%, Vocabulary 10%, Coherence 7%, Fillers 3%
        """
        st.subheader("üìà Performance Overview")
        
        # Calculate violations and proportional scoring
        total_questions = 3  # Total questions in interview
        total_violations = sum(len(r.get('violations', [])) for r in results)
        questions_answered = len([r for r in results if r.get("has_valid_data", False)])
        questions_with_violations = sum(1 for r in results if len(r.get('violations', [])) > 0)
        
        # Calculate maximum possible score
        max_possible_score = (questions_answered / total_questions) * 100
        
        # Show proportional scoring alert
        if questions_answered < total_questions:
            st.warning(f"‚ö†Ô∏è **Only {questions_answered}/{total_questions} questions answered** - Maximum possible score: **{max_possible_score:.1f}%**")
        
        if total_violations > 0:
            st.error(f"üö® **{total_violations} violation(s) detected across {questions_with_violations} question(s)** - Each violation: **-5% penalty**")
        
        valid_results = [r for r in results if r.get("has_valid_data", False)]
        
        if valid_results:
            # Calculate averages
            confs = [r.get("emotion_scores", {}).get("confidence", 0) for r in valid_results]
            accs = [r.get("accuracy", 0) for r in valid_results]
            fluencies = [r.get("fluency", 0) for r in valid_results]
            wpms = [r.get("wpm", 0) for r in valid_results]
            filler_counts = [r.get("filler_count", 0) for r in valid_results]
            
            # Enhanced metrics
            grammar_scores = [r.get("fluency_detailed", {}).get("grammar_score", 0) for r in valid_results]
            diversity_scores = [r.get("fluency_detailed", {}).get("lexical_diversity", 0) for r in valid_results]
            coherence_scores = [r.get("fluency_detailed", {}).get("coherence_score", 0) for r in valid_results]
            pause_ratios = [r.get("fluency_detailed", {}).get("pause_ratio", 0) for r in valid_results]
            speech_rate_norms = [r.get("fluency_detailed", {}).get("speech_rate_normalized", 0) for r in valid_results]
            
            avg_conf = np.mean(confs)
            avg_acc = np.mean(accs)
            avg_flu = np.mean(fluencies)
            avg_wpm = np.mean(wpms)
            avg_filler = np.mean(filler_counts)
            avg_grammar = np.mean(grammar_scores) if grammar_scores else 0
            avg_diversity = np.mean(diversity_scores) if diversity_scores else 0
            avg_coherence = np.mean(coherence_scores) if coherence_scores else 0
            avg_speech_norm = np.mean(speech_rate_norms) if speech_rate_norms else 0
            
            # Main metrics
            m1, m2, m3, m4, m5 = st.columns(5)
            with m1:
                st.markdown(f"<div class='metric-card'><h3>{avg_conf:.1f}%</h3><p>Avg Confidence (20%)</p></div>", unsafe_allow_html=True)
            with m2:
                st.markdown(f"<div class='metric-card'><h3>{avg_acc:.1f}%</h3><p>Avg Accuracy (25%)</p></div>", unsafe_allow_html=True)
            with m3:
                st.markdown(f"<div class='metric-card'><h3>{avg_flu:.1f}%</h3><p>Avg Fluency (25%)</p></div>", unsafe_allow_html=True)
            with m4:
                filler_status = "‚úÖ" if avg_filler <= 2 else "‚ö†Ô∏è"
                st.markdown(f"<div class='metric-card'><h3>{filler_status} {avg_filler:.1f}</h3><p>Avg Fillers (3%)</p></div>", unsafe_allow_html=True)
            with m5:
                wpm_status = "‚úÖ" if 140 <= avg_wpm <= 160 else "‚ö†Ô∏è"
                st.markdown(f"<div class='metric-card'><h3>{wpm_status} {avg_wpm:.1f}</h3><p>Avg WPM</p></div>", unsafe_allow_html=True)
            
            # Enhanced fluency breakdown
            st.markdown("### üó£Ô∏è Detailed Fluency Breakdown")
            st.caption("‚úÖ New Weights Applied: Grammar 10% | Vocabulary 10% | Coherence 7%")
            
            fm1, fm2, fm3, fm4, fm5 = st.columns(5)
            with fm1:
                st.markdown(f"<div class='metric-card'><h3>{avg_grammar:.1f}%</h3><p>Grammar (10%) ‚úèÔ∏è</p></div>", unsafe_allow_html=True)
            with fm2:
                st.markdown(f"<div class='metric-card'><h3>{avg_diversity:.1f}%</h3><p>Vocabulary (10%) üìö</p></div>", unsafe_allow_html=True)
            with fm3:
                st.markdown(f"<div class='metric-card'><h3>{avg_coherence:.1f}%</h3><p>Coherence (7%) üîó</p></div>", unsafe_allow_html=True)
            with fm4:
                avg_pause = np.mean(pause_ratios) if pause_ratios else 0
                st.markdown(f"<div class='metric-card'><h3>{avg_pause*100:.1f}%</h3><p>Pause Ratio ‚è∏Ô∏è</p></div>", unsafe_allow_html=True)
            with fm5:
                norm_status = "‚úÖ" if avg_speech_norm >= 0.9 else ("‚úì" if avg_speech_norm >= 0.7 else "‚ö†Ô∏è")
                st.markdown(f"<div class='metric-card'><h3>{norm_status} {avg_speech_norm:.2f}</h3><p>Speech Quality</p></div>", unsafe_allow_html=True)
            
            # Overall recommendation with PROPORTIONAL SCORING
            st.markdown("---")
            st.subheader("üéØ Overall Recommendation")
            
            if total_violations >= 5:
                st.error("‚ùå **Disqualified** - Multiple serious violations detected")
                st.info("Candidate showed pattern of policy violations during interview")
            else:
                # UPDATED WEIGHTED SCORING: Fluency 25%, Accuracy 25%, Confidence 20%, Grammar 10%, Vocabulary 10%, Coherence 7%, Fillers 3%
                overall_score = (
                    avg_conf * 0.20 +          # Confidence: 20%
                    avg_acc * 0.25 +           # Accuracy: 25%
                    avg_flu * 0.25 +           # Fluency: 25%
                    avg_grammar * 0.10 +       # Grammar: 10%
                    avg_diversity * 0.10 +     # Vocabulary: 10%
                    avg_coherence * 0.07 +     # Coherence: 7%
                    (100 - avg_filler * 10) * 0.03  # Fillers: 3%
                )
                
                # Apply proportional cap based on questions answered
                overall_score = (overall_score / 100) * max_possible_score
                
                # Violation penalty: -5% per violation
                violation_penalty = total_violations * 5
                final_score = max(0, overall_score - violation_penalty)
                
                col_rec1, col_rec2 = st.columns([1, 2])
                with col_rec1:
                    st.metric("Overall Score", f"{final_score:.1f}%", 
                             delta=f"Max: {max_possible_score:.1f}%",
                             help=f"Based on {questions_answered}/{total_questions} questions answered")
                    
                    if violation_penalty > 0:
                        st.metric("Violation Penalty", f"-{violation_penalty}%",
                                 delta=f"{total_violations} violation(s)")
                
                with col_rec2:
                    # Show proportional scoring explanation
                    if questions_answered < total_questions:
                        st.info(f"""
                        **üìä Proportional Scoring Applied:**
                        - Answered {questions_answered}/{total_questions} questions
                        - Maximum possible: {max_possible_score:.1f}%
                        - Your raw score: {overall_score:.1f}%
                        - After penalties: {final_score:.1f}%
                        """)
                    
                    if violation_penalty > 0:
                        st.warning(f"‚ö†Ô∏è Score reduced by {violation_penalty}% due to {total_violations} violation(s) (-5% each)")
                    
                    # Adjusted thresholds based on max_possible_score
                    threshold_80 = max_possible_score * 0.80
                    threshold_70 = max_possible_score * 0.70
                    threshold_60 = max_possible_score * 0.60
                    threshold_50 = max_possible_score * 0.50
                    
                    if final_score >= threshold_80:
                        st.success("‚úÖ **Exceptional Candidate** - Strong hire recommendation")
                        st.info("Outstanding communication, fluency, and technical competence")
                    elif final_score >= threshold_70:
                        st.success("‚úÖ **Strong Candidate** - Recommended for hire")
                        st.info("Excellent communication skills with minor areas for growth")
                    elif final_score >= threshold_60:
                        st.warning("‚ö†Ô∏è **Moderate Candidate** - Further evaluation recommended")
                        st.info("Good potential with notable room for improvement")
                    elif final_score >= threshold_50:
                        st.warning("‚ö†Ô∏è **Weak Candidate** - Significant concerns")
                        st.info("Below expectations in multiple areas")
                    else:
                        st.error("‚ùå **Not Recommended** - Does not meet standards")
                        st.info("Substantial improvement needed across all metrics")
            
            # Show scoring breakdown
            st.markdown("---")
            st.markdown("### üìä Scoring Breakdown")
            
            scoring_df = pd.DataFrame({
                'Component': ['Confidence', 'Accuracy', 'Fluency', 'Grammar', 'Vocabulary', 'Coherence', 'Fillers'],
                'Weight': ['20%', '25%', '25%', '10%', '10%', '7%', '3%'],
                'Avg Score': [
                    f"{avg_conf:.1f}%",
                    f"{avg_acc:.1f}%",
                    f"{avg_flu:.1f}%",
                    f"{avg_grammar:.1f}%",
                    f"{avg_diversity:.1f}%",
                    f"{avg_coherence:.1f}%",
                    f"{100 - avg_filler * 10:.1f}%"
                ],
                'Contribution': [
                    f"{avg_conf * 0.20:.1f}%",
                    f"{avg_acc * 0.25:.1f}%",
                    f"{avg_flu * 0.25:.1f}%",
                    f"{avg_grammar * 0.10:.1f}%",
                    f"{avg_diversity * 0.10:.1f}%",
                    f"{avg_coherence * 0.07:.1f}%",
                    f"{(100 - avg_filler * 10) * 0.03:.1f}%"
                ]
            })
            
            st.dataframe(scoring_df, use_container_width=True, hide_index=True)
            
            # Charts
            st.markdown("---")
            st.subheader("üìä Detailed Analytics")
            
            col_chart1, col_chart2 = st.columns(2)
            
            with col_chart1:
                st.write("**Performance by Question**")
                chart_data = pd.DataFrame({
                    'Question': [f"Q{i+1}" for i in range(len(valid_results))],
                    'Confidence': confs,
                    'Accuracy': accs,
                    'Fluency': fluencies
                })
                st.line_chart(chart_data.set_index('Question'))
            
            with col_chart2:
                st.write("**Component Contributions (New Weights)**")
                contribution_data = pd.DataFrame({
                    'Component': ['Confidence\n(20%)', 'Accuracy\n(25%)', 'Fluency\n(25%)', 'Grammar\n(10%)', 
                                 'Vocabulary\n(10%)', 'Coherence\n(7%)', 'Fillers\n(3%)'],
                    'Score': [
                        avg_conf * 0.20,
                        avg_acc * 0.25,
                        avg_flu * 0.25,
                        avg_grammar * 0.10,
                        avg_diversity * 0.10,
                        avg_coherence * 0.07,
                        (100 - avg_filler * 10) * 0.03
                    ]
                })
                st.bar_chart(contribution_data.set_index('Component'))
    
    def display_detailed_results(self, results):
        """Display detailed question-by-question analysis"""
        st.markdown("---")
        st.subheader("üìã Question-by-Question Analysis")
        
        for i, r in enumerate(results):
            decision = r.get('hire_decision', 'N/A')
            fluency_level = r.get('fluency_level', 'N/A')
            violations = r.get('violations', [])
            violation_badge = f"‚ö†Ô∏è {len(violations)} violation(s)" if violations else "‚úÖ Clean"
            filler_count = r.get('filler_count', 0)
            
            with st.expander(f"Q{i+1}: {r.get('question', '')[:60]}... ‚Äî {decision} | {violation_badge} | Fluency: {fluency_level}", expanded=False):
                # Display violations
                if violations:
                    st.error(f"**üö® {len(violations)} Violation(s) Detected (-5% each)**")
                    self.display_violation_images(violations)
                    st.markdown("---")
                
                col_vid, col_txt = st.columns([2, 3])
                
                with col_vid:
                    if os.path.exists(r.get('video_path', '')):
                        st.video(r['video_path'])
                
                with col_txt:
                    st.markdown(f"**üìã Question:** {r.get('question', '')}")
                    st.markdown("**üí¨ Transcript:**")
                    if self.is_valid_transcript(r.get('transcript', '')):
                        st.text_area("", r['transcript'], height=80, disabled=True, key=f"t_{i}", label_visibility="collapsed")
                    else:
                        st.error(r.get('transcript', 'No transcript'))
                    
                    # Main metrics
                    m1, m2, m3, m4 = st.columns(4)
                    with m1:
                        st.metric("üòä Confidence", f"{r.get('emotion_scores', {}).get('confidence', 0)}%")
                        st.metric("üìä Accuracy", f"{r.get('accuracy', 0)}%")
                    with m2:
                        st.metric("üò∞ Nervousness", f"{r.get('emotion_scores', {}).get('nervousness', 0)}%")
                        st.metric("üó£Ô∏è Fluency", f"{r.get('fluency', 0)}%")
                    with m3:
                        st.metric("üö´ Filler Words", filler_count)
                        st.metric("üò¥ Blinks", f"{r.get('blink_count', 0)}")
                    with m4:
                        st.metric("üëî Outfit", r.get('outfit', 'Unknown'))
                        st.metric("üí¨ WPM", f"{r.get('wpm', 0)}")
                    
                    # Enhanced fluency breakdown
                    fluency_detailed = r.get('fluency_detailed', {})
                    if fluency_detailed:
                        st.markdown("---")
                        st.markdown("**üìä Accurate Fluency Analysis:**")
                        
                        fcol1, fcol2, fcol3 = st.columns(3)
                        with fcol1:
                            st.write(f"**Grammar (10%):** {fluency_detailed.get('grammar_score', 0):.0f}% ‚úèÔ∏è")
                            st.write(f"**Errors:** {fluency_detailed.get('grammar_errors', 0)}")
                            st.write(f"**Vocabulary (10%):** {fluency_detailed.get('lexical_diversity', 0):.0f}% üìö")
                        with fcol2:
                            st.write(f"**Coherence (7%):** {fluency_detailed.get('coherence_score', 0):.0f}% üîó")
                            st.write(f"**Pauses:** {fluency_detailed.get('num_pauses', 0)}")
                            st.write(f"**Pause Ratio:** {fluency_detailed.get('pause_ratio', 0)*100:.1f}% ‚è∏Ô∏è")
                        with fcol3:
                            speech_norm = fluency_detailed.get('speech_rate_normalized', 0)
                            st.write(f"**Speech Quality:** {speech_norm:.2f}")
                            st.write(f"**Fluency Level:** {r.get('fluency_level', 'N/A')}")
                            st.write(f"**Filler Ratio (3%):** {fluency_detailed.get('filler_ratio', 0)*100:.1f}%")
                        
                        # Show detailed word counts
                        detail_metrics = fluency_detailed.get('detailed_metrics', {})
                        if detail_metrics:
                            st.markdown("**üìà Word Analysis:**")
                            st.caption(f"Total: {detail_metrics.get('total_words', 0)} | "
                                     f"Meaningful: {detail_metrics.get('meaningful_words', 0)} | "
                                     f"Unique: {detail_metrics.get('unique_words', 0)} | "
                                     f"Fillers: {detail_metrics.get('filler_words_detected', 0)}")
                            
                            if detail_metrics.get('stopword_filtered'):
                                st.caption("‚úÖ Stopword filtering applied")
                    
                    st.markdown("---")
                    st.markdown(f"**Decision:** {decision}")
                    st.markdown("**Reasons:**")
                    for reason in r.get('hire_reasons', []):
                        st.write(f"‚Ä¢ {reason}")
    
    def export_results_csv(self, results):
        """Export results to CSV - WITH PROPORTIONAL SCORING"""
        export_data = []
        total_questions = 3
        questions_answered = len([r for r in results if r.get("has_valid_data", False)])
        max_possible_score = (questions_answered / total_questions) * 100
        
        for i, r in enumerate(results):
            fluency_detailed = r.get('fluency_detailed', {})
            violations = r.get('violations', [])
            detail_metrics = fluency_detailed.get('detailed_metrics', {})
            improvements = r.get('improvements_applied', {})
            
            export_data.append({
                "Question_Number": i + 1,
                "Question": r.get("question", ""),
                "Transcript": r.get("transcript", ""),
                "Violations_Count": len(violations),
                "Violation_Details": "; ".join([v['reason'] for v in violations]),
                "Violation_Penalty_Percent": len(violations) * 5,
                "Max_Possible_Score": max_possible_score,
                "Questions_Answered": questions_answered,
                "Total_Questions": total_questions,
                "Confidence": r.get("emotion_scores", {}).get("confidence", 0),
                "Confidence_Weight": "20%",
                "Nervousness": r.get("emotion_scores", {}).get("nervousness", 0),
                "Accuracy": r.get("accuracy", 0),
                "Accuracy_Weight": "25%",
                "Fluency_Score": r.get("fluency", 0),
                "Fluency_Weight": "25%",
                "Fluency_Level": r.get("fluency_level", ""),
                "Speech_Rate_WPM": fluency_detailed.get("speech_rate", 0),
                "Speech_Rate_Normalized": fluency_detailed.get("speech_rate_normalized", 0),
                "Grammar_Score": fluency_detailed.get("grammar_score", 0),
                "Grammar_Weight": "10%",
                "Grammar_Errors": fluency_detailed.get("grammar_errors", 0),
                "Lexical_Diversity": fluency_detailed.get("lexical_diversity", 0),
                "Vocabulary_Weight": "10%",
                "Coherence_Score": fluency_detailed.get("coherence_score", 0),
                "Coherence_Weight": "7%",
                "Pause_Ratio": fluency_detailed.get("pause_ratio", 0),
                "Avg_Pause_Duration": fluency_detailed.get("avg_pause_duration", 0),
                "Num_Pauses": fluency_detailed.get("num_pauses", 0),
                "Filler_Word_Count": fluency_detailed.get("filler_count", 0),
                "Filler_Word_Ratio": fluency_detailed.get("filler_ratio", 0),
                "Fillers_Weight": "3%",
                "Total_Words": detail_metrics.get("total_words", 0),
                "Meaningful_Words": detail_metrics.get("meaningful_words", 0),
                "Unique_Words": detail_metrics.get("unique_words", 0),
                "Unique_Meaningful_Words": detail_metrics.get("unique_meaningful_words", 0),
                "Blink_Count": r.get("blink_count", 0),
                "Outfit": r.get("outfit", ""),
                "Outfit_Confidence": r.get("outfit_confidence", 0),
                "Hire_Decision": r.get("hire_decision", ""),
                "Accurate_Metrics_Only": improvements.get("no_fake_metrics", False),
                "Proportional_Scoring_Applied": True,
                "Stopword_Filtering": improvements.get("stopword_filtering", False),
                "Quality_Weighted_Emotions": improvements.get("quality_weighted_emotions", False),
                "BERT_Coherence": improvements.get("bert_coherence", False),
                "Content_Similarity": improvements.get("content_similarity_matching", False),
                "Filler_Word_Detection": improvements.get("filler_word_detection", False)
            })
        
        df = pd.DataFrame(export_data)
        csv = df.to_csv(index=False)
        return csv
    
    def render_dashboard(self, results):
        """Render complete results dashboard - PROPORTIONAL SCORING"""
        if not results:
            st.info("üî≠ No results yet. Complete some questions first.")
            return
        
        # Show accuracy badge and NEW WEIGHTS
        if results:
            improvements = results[0].get("improvements_applied", {})
            
            st.success("‚úÖ **PROPORTIONAL SCORING ACTIVE** | Scores based on questions answered")
            
            col_weight1, col_weight2 = st.columns(2)
            with col_weight1:
                st.info("""
                **üéØ NEW EVALUATION WEIGHTS:**
                - Fluency: **25%**
                - Accuracy: **25%**
                - Confidence: **20%**
                - Grammar: **10%**
                """)
            with col_weight2:
                st.info("""
                **üìä ADDITIONAL WEIGHTS:**
                - Vocabulary: **10%**
                - Coherence: **7%**
                - Fillers: **3%**
                - Violations: **-5% each**
                """)
            
            if improvements.get('no_fake_metrics'):
                active_improvements = []
                if improvements.get('stopword_filtering'):
                    active_improvements.append("üîç Stopword Filtering")
                if improvements.get('quality_weighted_emotions'):
                    active_improvements.append("‚öñÔ∏è Quality-Weighted Emotions")
                if improvements.get('content_similarity_matching'):
                    active_improvements.append("üîó Content Similarity")
                if improvements.get('bert_coherence'):
                    active_improvements.append("üß† BERT Coherence")
                if improvements.get('filler_word_detection'):
                    active_improvements.append("üö´ Filler Word Detection")
                if improvements.get('grammar_error_count'):
                    active_improvements.append("‚úèÔ∏è Grammar Error Count")
                
                if active_improvements:
                    st.info("**Active Improvements:** " + " | ".join(active_improvements))
        
        # Performance overview (with proportional scoring)
        self.display_performance_overview(results)
        
        # Detailed results
        self.display_detailed_results(results)
        
        # Export option
        st.markdown("---")
        col_export1, col_export2 = st.columns(2)
        
        with col_export1:
            if st.button("üì• Download Proportional Results as CSV", use_container_width=True):
                csv = self.export_results_csv(results)
                st.download_button(
                    "üíæ Download CSV",
                    csv,
                    f"interview_results_proportional_{time.strftime('%Y%m%d_%H%M%S')}.csv",
                    "text/csv",
                    use_container_width=True
                )
        
        with col_export2:
            # Show scoring details
            if st.button("‚ÑπÔ∏è View Scoring Details", use_container_width=True):
                with st.expander("üìä Proportional Scoring System", expanded=True):
                    st.markdown("""
                    ### üéØ How Proportional Scoring Works
                    
                    **Example Scenario:**
                    - Total questions: **3**
                    - Questions answered: **1** (2 violations)
                    - Maximum possible score: **33.33%** (1/3 √ó 100%)
                    
                    **Scoring Process:**
                    
                    1. **Calculate Raw Score** (using new weights):
                       ```
                       Raw Score = 
                           Confidence √ó 20% +
                           Accuracy √ó 25% +
                           Fluency √ó 25% +
                           Grammar √ó 10% +
                           Vocabulary √ó 10% +
                           Coherence √ó 7% +
                           (100 - Fillers√ó10) √ó 3%
                       ```
                    
                    2. **Apply Proportional Cap**:
                       ```
                       Capped Score = (Raw Score / 100) √ó Max Possible Score
                       Example: (85 / 100) √ó 33.33% = 28.33%
                       ```
                    
                    3. **Subtract Violation Penalties**:
                       ```
                       Final Score = Capped Score - (Violations √ó 5%)
                       Example: 28.33% - (2 √ó 5%) = 18.33%
                       ```
                    
                    ---
                    
                    ### üìä NEW Weight Distribution
                    
                    | Component | Weight | Rationale |
                    |-----------|--------|-----------|
                    | **Fluency** | 25% | Core communication skill |
                    | **Accuracy** | 25% | Content relevance & understanding |
                    | **Confidence** | 20% | Professional presence |
                    | **Grammar** | 10% | Language proficiency |
                    | **Vocabulary** | 10% | Verbal diversity |
                    | **Coherence** | 7% | Logical flow |
                    | **Fillers** | 3% | Speaking polish |
                    | **Violations** | -5% each | Integrity penalty |
                    
                    ---
                    
                    ### ‚öñÔ∏è Fair Assessment Principles
                    
                    ‚úÖ **Candidates are only scored on what they attempted**
                    - If 1/3 questions answered ‚Üí Max score is 33.33%
                    - If 2/3 questions answered ‚Üí Max score is 66.67%
                    - If 3/3 questions answered ‚Üí Max score is 100%
                    
                    ‚úÖ **Violations impact ALL candidates equally**
                    - Each violation = -5% penalty
                    - Applied AFTER proportional calculation
                    - Transparent and consistent
                    
                    ‚úÖ **Performance thresholds adjust proportionally**
                    - "Exceptional" = 80% of max possible
                    - "Strong" = 70% of max possible
                    - "Moderate" = 60% of max possible
                    - "Weak" = 50% of max possible
                    
                    ---
                    
                    ### üö® Violation Impact Examples
                    
                    **Scenario 1:** Full completion (3/3 questions)
                    - Max possible: 100%
                    - Raw score: 85%
                    - 2 violations: -10%
                    - **Final: 75%** ‚Üí Strong candidate
                    
                    **Scenario 2:** Partial completion (1/3 questions)
                    - Max possible: 33.33%
                    - Raw score: 85% ‚Üí Capped at 28.33%
                    - 2 violations: -10%
                    - **Final: 18.33%** ‚Üí Not recommended
                    
                    **Key Insight:** Violations hurt proportionally more when fewer questions are completed, incentivizing full participation and compliance.
                    
                    ---
                    
                    ### ‚úÖ Why This System is Fair
                    
                    1. **Merit-Based**: Score reflects actual performance
                    2. **Transparent**: Clear calculation methodology
                    3. **Consistent**: Same rules for all candidates
                    4. **Proportional**: Penalizes incomplete attempts appropriately
                    5. **Integrity-Focused**: Violations have meaningful consequences
                    6. **Accurate**: Only uses verified, real metrics
                    """)


###